# GAN

- The objecive of MinMax game

![Untitled](GAN%207ec9ed49b4314d75a25f075cfb41207a/Untitled.png)

![Untitled](Untitled%205.png)

- Goal: the discriminator is unable to differentiate between the two distributions. i.e. **D(x) = 1/2**
- In practice, equation 1 may not provide sufficient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high confidence. ⇒ **log(1-D(G(z)) saturates** in this case.
    - —> Thus, rather than training G to minimize log(1-D(G(z)), we can train G to maximize logD(G(z)). This objective function results in the same fixed point of the dynamics of G and D but provides **much stronger gradients early in learning.**
- Proof:

![Untitled](GAN%207ec9ed49b4314d75a25f075cfb41207a/Untitled%201.png)

![Untitled](GAN%207ec9ed49b4314d75a25f075cfb41207a/Untitled%202.png)

- **p_z(z) ⇒ p_g(x)** since G is fixed and the noise distribution will eventually **follow the probability distribution** of generator g(x). Similarly, g(z) ⇒ x since g(z) will produce x.
    - Indeed, this x could be called a quasi-x or even hat x, but we assume that this quasi-x will eventually follow the input x distribution through the minmax game (as it learns the parameters from discriminator that updates parameter k steps and generator updates for 1 step — look at the algorithm presented in the summary)

![Untitled](GAN%207ec9ed49b4314d75a25f075cfb41207a/Untitled%203.png)

- **Supp(p_data) ⇒ p_data(x) ≠ 0.** Hence, the discriminator does not need to be defined outside of the given union domain of S_upp(p_data) U S_upp(p_g)
- Training objecive for **D ≈ maximizing the log-likelihood** for estimating the **conditional prob.** **P(Y=y|x)** with y=1 or 0 for real or fake.

![Untitled](GAN%207ec9ed49b4314d75a25f075cfb41207a/Untitled%204.png)

![Untitled](GAN%207ec9ed49b4314d75a25f075cfb41207a/Untitled%205.png)